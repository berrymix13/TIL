# 검색어를 입력해서 네이버 정보 가져오기

- 사용 모듈

  ```python
  from selenium import webdriver
  from bs4 import BeautifulSoup
  from urllib.parse import quote
  import requests
  import time
  import pandas as pd
  import re
  ```

  * `urllib.perse`  :   URL 구문 분석과 인용(quoting)에 쓰이는 함수. 아스키코드 형식이 하닌 글자를 url에 인코딩 시킬 때 사용한다.

## url 생성 함수

```python
def sch_url(schtext, schpage=1):
    url = "https://section.blog.naver.com/Search/Post.naver?"
    url += "pageNo=" +str(schpage)
    url += "&rangeType=ALL&orderBy=sim&keyword=" + quote(schtext)
    
    return url
```

```python
schtext ='국내 여행'
url = sch_url(schtext, 2)
print(url)
```

#### 하이퍼링크 가져오기

검색 결과에서 각각의 포스트에 있는 하이퍼링크를 가져오자.

```python
driver = webdriver('c:/pydata/chromedriver.exe')
driver.get(url)

html = driver.page_source
soup = bs(html, 'html.parser')
a_soup = soup.find_all('a', class_'desc_inner')

url_lst = []
for a in a_soup:
    url_lst.append(a['herf'])
    
url_lst
```

## iframe에서 내용 크롤링 해오기

#### iframe이란?

- 네이버 블로그 처럼 f12를 눌렀을 때 iframe이 떠 있는 구조
- 내용들을 바로 가져올 수 없는 출력물을 보여주는 화면이다.

```python
for link in url_lst:
    html = requests.get(link)
    soup = bs(html.content, 'html.parser')
    new_url = "http://blog.naver.com" + soup.find('iframe', id='mainFrame')['src']   # 대괄호로 속성 값을 바로 뽑아낼 수 있다.
    
    html = requests.get(new_url)
    soup = bs(html.text, 'html.parser')
```

- requests에는 text와 content라는 명령어가 존재한다.
- `html.text` : 수신한 btye 단위의 데이터를 자동으로 디코딩하여 사람이 읽기 쉽게 만들어줌.
- `html.content` : 수신한 btye 단위 데이터를 그대로 보여줘 프로그램이 처리하기 편하게 만듦. 사진이나 파일등을 전송 받을 때 text를 사용하면 데이터가 손상되기 때문에 content를 사용해줘야 됨.

```python
for link in url_lst:
    driver.get(link)
    time.sleep(1)
    
    driver.switch_to.frame('mainFrame')   # iframe 활성화
    html = driver.page_source
    soup = bs(html, 'html.parser')
    
driver.quit()
```

## 1 페이지부터 10페이지까지의 블로그 내용 가져오기

### 정규식 표현

[0-9] 또는 [a-zA-Z]등은 무척 자주 사용하는 정규 표현식이다.

- `\d `    :    숫자와 매치된 `[0-9]`와 동일한 의미
- `\D`    :   숫자가 아닌 것과 매치. `[^0-9]`
- `\s`    :   whitespace 와 매치. `[ \t\n\r\f\v]`와 동일한 표현.
- `\S`    :   whitespace가 아닌 것과 매치
- `\w`    :   문자+숫자와 매치. `[a-zA-Z]`
- `\W`    :   문자+숫자 이외의 것. `[^a-zA-z]`

### URL 만들어 제목과 하이퍼링크 가져오기

1 page부터 10페이지까지 가져오는 함수에는 위에서 만든 함수의 spage부분을 for문으로 돌리면 된다.  각 페이지마다 블로그 글의 제목과 하이퍼링크를 먼저 가져오자.

```python
def sch_url1(schtext, stpage = 1, enpage = 5):
    url_lst = []
    for i in range(stpage, enpage + 1):
        url =  "https://section.blog.naver.com/Search/Post.naver?"
        url += "pageNo=" +str(i)
        url += "&rangeType=ALL&orderBy=sim&keyword=" + quote(schtext)
        
        driver = webdriver.Chrome('c:/pydata/webdriver.exe')
        driver.get(url)
        time.sleep(1)
        
        html = driver.page_source
        soup = bs(html, 'html.parser')
        title_soup = soup.find_all('span', class_='title')
        a_soup = soup.ifnd_all('a',class_='desc_inner')
        
        for t, a in zip(title_soup, a_soup):
            url_lst.append([t.text,a['herf']])
    return url_lst
```

### 하이퍼링크를 통해 iframe에서 블로그 내용 가져오기.

```python
def url_text(url_lst):
    txt_lst = []
    for link in url_lst:
        html = requests.get(link[1])
        soup = bs(html.content, 'html.parser')
        new_url = 'https://blog.naver.com' + find('iframe', id = 'mainFrame')['src']
        
        html = requests.get(new_url)
        soup = bs(html.text, 'html.parser')
        
        try:
            body_txt = soup.find('div', class_='se-main-container')
        except:
            body_txt=soup.find('div', id="postViewArea").text
        body_text = re.sub('[^0-9a-zA-Z가-힣 ]', '',body_text)
        
        txt_lst.append([link[0], body_text])
    return txt_lst
```

`import re` 는 정규 표현식을 사용하기 위한 모듈이다. 

- `re.sub()`   :   문자열에 맞는 패턴을 교체할 문자열로 교체해준다. 
- re.sub( `정규표현식` ,  `교체할 문자열` ,  `문자열` )
